{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import loadtxt\n",
    "from sklearn import preprocessing \n",
    "from fuzzywuzzy import fuzz\n",
    "import re, math\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Various Function Definitions for Feature Engineering\n",
    "class FeatureEngineeringFunctions:\n",
    "    \n",
    "    #Given question2 column is not entirely string\n",
    "    #Converting question2 to string and in the test set \n",
    "    #Even question1 column might have such problem hence converting that too    \n",
    "    def convert_to_string(self, train):\n",
    "        train['question1'] = train['question1'].astype(str)\n",
    "        train['question2'] = train['question2'].astype(str)\n",
    "        train['lem_question1'] = train['lem_question1'].astype(str)\n",
    "        train['lem_question2'] = train['lem_question2'].astype(str)\n",
    "\n",
    "\n",
    "    #Porter Stemming\n",
    "    def portertok(self, text):\n",
    "        import nltk\n",
    "        from nltk.stem.porter import PorterStemmer\n",
    "        porter = PorterStemmer()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return \" \".join(porter.stem(word) for word in tokens) \n",
    "\n",
    "\n",
    "    #Counts of common words\n",
    "    #First convert to string then lower case then split on the basis of space then create a set\n",
    "    #then check the common elements of both set and take count of it\n",
    "    def count_common(self, question):\n",
    "        q1, q2 = question\n",
    "        return len((set(str(q1).lower().split(' ')) & set(str(q2).lower().split(' '))))/len((set(str(q1).lower().split(' ')).union(set(str(q2).lower().split(' ')))))\n",
    "\n",
    "\n",
    "    #Comparing each question pair\n",
    "    def check_each_pair(self, question1, question2):\n",
    "        print(question1, question2)\n",
    "\n",
    "    #Total unique words in both question pair\n",
    "    def total_unique_words(self, question):\n",
    "        q1, q2 = question\n",
    "        return len(set(str(q1)).union(set(str(q2))))\n",
    "\n",
    "\n",
    "    #Counts the total number of words present\n",
    "    def words_count(self, question):\n",
    "        return len(str(question).split(' '))\n",
    "\n",
    "\n",
    "    #Gives the length of the string\n",
    "    def length(self, question):\n",
    "        return len(str(question))\n",
    "\n",
    "    #Word count ratio\n",
    "    def word_count_ratio(self, question):\n",
    "        q1, q2 = question\n",
    "        l1 = len(set(str(q1)))*1.0 \n",
    "        l2 = len(set(str(q2)))\n",
    "        if l2 == 0:\n",
    "            return np.nan\n",
    "        if l1 / l2:\n",
    "            return l2 / l1\n",
    "        else:\n",
    "            return l1 / l2\n",
    "\n",
    "    #Remove Punctuations\n",
    "    def remove_punc(self, question):\n",
    "        return question.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Same First Word\n",
    "    def same_first_word(self, question):\n",
    "        q1, q2 = question\n",
    "        return floatset(set(str(q1).lower().split(' '))[0] == set(str(q2).lower().split(' '))[0])\n",
    "    \n",
    "    #Get the cosine values for each question pair\n",
    "    def get_cosine(myDataFrame):\n",
    "        WORD = re.compile(r'\\w+')\n",
    "\n",
    "        vec1 = text_to_vector(str(myDataFrame['lem_question1']))\n",
    "        vec2 = text_to_vector(str(myDataFrame['lem_question2']))\n",
    "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)        \n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float(numerator) / denominator\n",
    "        \n",
    "    #Converts text to vector    \n",
    "    def text_to_vector(text):\n",
    "        words = WORD.findall(text)\n",
    "        return Counter(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "#Applying Feature Engineering\n",
    "class FeatureEngineering:\n",
    "    \n",
    "    def get_feature_engineered_data(self, flag):\n",
    "\n",
    "        #Creating an object of Data Exploration \n",
    "        obj = FeatureEngineeringFunctions()\n",
    "        \n",
    "        if flag==1:\n",
    "            train = pd.read_csv('../LematizedFiles/trainlem.csv', engine='python')\n",
    "        else:\n",
    "            train = pd.read_csv('../LematizedFiles/testlem.csv',engine='python')\n",
    "\n",
    "        obj.convert_to_string(train)\n",
    "\n",
    "        #Data Parameter used for some feature engineering attributes\n",
    "        EPSILON = 0.0000001\n",
    "\n",
    "        #Map works element wise on a series\n",
    "        #Apply works row/column wise on a dataframe\n",
    "        train['q1_word_num'] = train['question1'].map(obj.words_count)\n",
    "        train['q2_word_num'] = train['question2'].map(obj.words_count)\n",
    "\n",
    "        train['q1_length'] = train['question1'].map(obj.length)\n",
    "        train['q2_length'] = train['question2'].map(obj.length)\n",
    "\n",
    "\n",
    "        train['word_num_difference'] = abs(train.q1_word_num - train.q2_word_num)\n",
    "        train['length_difference'] = abs(train.q1_length - train.q2_length)\n",
    "\n",
    "        train['q1_has_fullstop'] = train.question1.apply(lambda x: int('.' in x))\n",
    "        train['q2_has_fullstop'] = train.question2.apply(lambda x: int('.' in x))\n",
    "\n",
    "        train['q1_digit_count'] = train.question1.apply(lambda question: sum([word.isdigit() for word in question]))\n",
    "        train['q2_digit_count'] = train.question2.apply(lambda question: sum([word.isdigit() for word in question]))\n",
    "        train['digit_count_difference'] = abs(train.q1_digit_count - train.q2_digit_count)\n",
    "\n",
    "        train['q1_capital_char_count'] = train.question1.apply(lambda question: sum([word.isupper() for word in question]))\n",
    "        train['q2_capital_char_count'] = train.question2.apply(lambda question: sum([word.isupper() for word in question]))\n",
    "        train['capital_char_count_difference'] = abs(train.q1_capital_char_count - train.q2_capital_char_count)\n",
    "\n",
    "        train['q1_has_math_expression'] = train.question1.apply(lambda x: int('[math]' in x))\n",
    "        train['q2_has_math_expression'] = train.question2.apply(lambda x: int('[math]' in x))      \n",
    "\n",
    "        train['common_words'] = train[['question1', 'question2']].apply(obj.count_common, axis=1)\n",
    "        train['lem_common_words'] = train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1)\n",
    "\n",
    "        train['log_word_share'] = np.log(train[['question1', 'question2']].apply(obj.count_common, axis=1) + EPSILON)\n",
    "        train['lem_log_word_share'] = np.log(train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1) + EPSILON)\n",
    "\n",
    "        train['word_share_squared'] = (train[['question1', 'question2']].apply(obj.count_common, axis=1) ** 2)\n",
    "        train['lem_word_share_squared'] = (train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1) ** 2)\n",
    "\n",
    "\n",
    "        train['word_share_sqrt'] = np.sqrt(train[['question1', 'question2']].apply(obj.count_common, axis=1))\n",
    "        train['lem_word_share_sqrt'] = np.sqrt(train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1))\n",
    "\n",
    "        train['log_length_difference'] = np.log(train.length_difference + EPSILON)\n",
    "        train['length_difference_squared'] = train.length_difference ** 2\n",
    "        train['length_difference_sqrt'] = np.sqrt(train.length_difference)\n",
    "\n",
    "        train['log_lem_tfidf'] = np.log(train.lem_tfidf_word_match + EPSILON)\n",
    "        train['lem_tfidf_squared'] = train.lem_tfidf_word_match ** 2\n",
    "        train['lem_tfidf_sqrt'] = np.sqrt(train.lem_tfidf_word_match)\n",
    "\n",
    "        train['total_unique_words'] = train[['question1', 'question2']].apply(obj.total_unique_words, axis=1)\n",
    "        train['word_count_ratio'] = train[['question1', 'question2']].apply(obj.word_count_ratio, axis=1)\n",
    "\n",
    "       \n",
    "        train['fuzz_qratio'] = train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_WRatio'] = train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_ratio'] = train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_token_set_ratio'] = train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_token_sort_ratio'] = train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_token_set_ratio'] = train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_token_sort_ratio'] = train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "\n",
    "        train['cosine_score'] = train.apply(get_cosine, axis=1, raw=True)\n",
    "\n",
    "        features = ['q1_word_num', 'q2_word_num', 'word_num_difference', 'q1_length', 'q2_length',\n",
    "                'length_difference', 'q1_has_fullstop', 'q2_has_fullstop', 'q1_digit_count', 'q2_digit_count',\n",
    "                'digit_count_difference', 'q1_capital_char_count', 'q2_capital_char_count',\n",
    "                'capital_char_count_difference', 'q1_has_math_expression', 'q2_has_math_expression', \n",
    "                'log_length_difference', 'log_word_share', 'word_share_squared', 'word_share_sqrt', 'length_difference_squared', 'length_difference_sqrt', 'common_words',\n",
    "                'lem_common_words','lem_log_word_share','lem_word_share_squared','lem_word_share_sqrt','tfidf_word_match',\n",
    "                'lem_tfidf_word_match', 'intersection_count', 'log_lem_tfidf','lem_tfidf_squared','lem_tfidf_sqrt',\n",
    "                   'total_unique_words', 'word_count_ratio','fuzz_qratio','fuzz_WRatio','fuzz_partial_ratio','fuzz_partial_token_set_ratio',\n",
    "                   'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio','fuzz_token_sort_ratio', 'cosine_score',\n",
    "                   'q1_freq', 'q2_freq']\n",
    "\n",
    "\n",
    "        if flag==1:\n",
    "            target = 'is_duplicate'\n",
    "            X = train[features]\n",
    "            Y = train[target]\n",
    "            return X,Y\n",
    "        else:\n",
    "            X = train[features]\n",
    "            return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class StartingClass:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Feature Engineering\n",
    "        obj = FeatureEngineering()\n",
    "        X_total,Y_total = obj.get_feature_engineered_data(1)\n",
    "        test_X = obj.get_feature_engineered_data(0)\n",
    "        \n",
    "        X_total['is_duplicate'] = Y_total\n",
    "        \n",
    "        X_total.to_csv('../FeatureEngineeringFiles/featured_train.csv', index=False)\n",
    "        test_X.to_csv('../FeatureEngineeringFiles/featured_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a22689291540>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#Read csv files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStartingClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-84b6de3fa03a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#Feature Engineering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mX_total\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_engineered_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtest_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_engineered_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f6b66ce3a334>\u001b[0m in \u001b[0;36mget_feature_engineered_data\u001b[1;34m(self, flag)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../LematizedFiles/trainlem.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../LematizedFiles/testlem.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    980\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'skipfooter not supported for iteration'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 982\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'as_recarray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   2115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2117\u001b[1;33m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2118\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   2850\u001b[0m                         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2851\u001b[0m                             new_row = self._next_iter_line(\n\u001b[1;32m-> 2852\u001b[1;33m                                 row_num=self.pos + rows + 1)\n\u001b[0m\u001b[0;32m   2853\u001b[0m                             \u001b[0mrows\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_next_iter_line\u001b[1;34m(self, row_num)\u001b[0m\n\u001b[0;32m   2576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2577\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2578\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2579\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2580\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn_bad_lines\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Starting Point\n",
    "if __name__ == '__main__':\n",
    "    #Read csv files\n",
    "    obj = StartingClass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

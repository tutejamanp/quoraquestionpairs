{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas_ml as pdml\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning null values of TF-IDF and lemmatizated features\n",
    "class DataCleaning:\n",
    "    \n",
    "    #Cleaning the first half of training dataset\n",
    "    def clean_data(self, X):\n",
    "        X['tfidf_word_match'] = X['tfidf_word_match'].fillna(0)\n",
    "        X['lem_tfidf_word_match'] = X['lem_tfidf_word_match'].fillna(0)\n",
    "        X['log_lem_tfidf'] = X['log_lem_tfidf'].fillna(0)\n",
    "        X['lem_tfidf_squared'] = X['lem_tfidf_squared'].fillna(0)\n",
    "        X['lem_tfidf_sqrt'] = X['lem_tfidf_sqrt'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameter Tuning for XGBoost based on the dataset\n",
    "class ParameterTuning:\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        self.get_best_learning_rate(X, Y)\n",
    "        self.get_best_depth_weight(X, Y)\n",
    "        \n",
    "        \n",
    "    #Best learning rate 0.3 for the first XGBoost    \n",
    "    def get_best_learning_rate(self, X, Y):\n",
    "    \n",
    "        n = Y.shape\n",
    "        Y=np.array(Y).reshape((n[0],))\n",
    "\n",
    "        model = XGBClassifier()\n",
    "        learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "        param_grid = dict(learning_rate=learning_rate)\n",
    "        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "        grid_result = grid_search.fit(X, Y)\n",
    "        # summarize results\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        \n",
    "        #Best max depth 5 and min_child_weight 3 for the first XGBoost\n",
    "        def get_best_depth_weight(self, X, Y):\n",
    "\n",
    "            n = Y.shape\n",
    "            Y=np.array(Y).reshape((n[0],))\n",
    "\n",
    "            param_test1 = {\n",
    "             'max_depth':range(3,10,1),\n",
    "             'min_child_weight':range(1,6,1)\n",
    "            }\n",
    "\n",
    "            gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.3, n_estimators=140, max_depth=5,\n",
    "             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "             objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "             param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "            gsearch1.fit(X,Y)\n",
    "            gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Layer Model Training using Stacking\n",
    "#Layer 1 - Random Forest + XGBoost\n",
    "#Layer2 - XGBoost\n",
    "class ModelTraining:\n",
    "    \n",
    "    xgbModel = XGBClassifier()\n",
    "    randomModel = RandomForestClassifier()\n",
    "    stackedModel = XGBClassifier()\n",
    "        \n",
    "    def layer_one_model_training(self, X, Y, val, test_X):\n",
    "        \n",
    "        #Training a Random Forest Model based on training data set\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=50,\n",
    "            n_jobs=8)\n",
    "        self.randomModel = model.fit(X, Y)\n",
    "        \n",
    "        #Training a XGB Model based on training data set\n",
    "        #Parameters used are the ones we got from parameter tuning\n",
    "        #obj = ParameterTuning(X, Y)\n",
    "        model1 = XGBClassifier(\n",
    "         learning_rate=0.3,\n",
    "         n_estimators=140,\n",
    "         max_depth=5,\n",
    "         min_child_weight=3,\n",
    "         gamma=0,\n",
    "         subsample=0.8,\n",
    "         colsample_bytree=0.8,\n",
    "         objective= 'binary:logistic',\n",
    "         nthread=4,\n",
    "         scale_pos_weight=1,\n",
    "         seed=27)\n",
    "        self.xgbModel = model1.fit(X, Y, verbose=True)\n",
    "\n",
    "\n",
    "        #Forest Values for validation and test data set\n",
    "        X2 = val.iloc[0:,0:43]\n",
    "        Y2 = val.iloc[0:,43:44]\n",
    "        \n",
    "        forest_val_value = model.predict_proba(X2)[:, 1]\n",
    "        \n",
    "        #XGB Values for validation and test data set\n",
    "        xgb_val_value = model1.predict_proba(X2)[:, 1]\n",
    "        self.layer_two_model_training(forest_val_value, xgb_val_value, forest_test_value, xgb_test_value, Y2, test_X)\n",
    "        \n",
    "        \n",
    "    def layer_two_model_training(self, forest_val_value, xgb_val_value, forest_test_value, xgb_test_value, Y2, test_X):\n",
    "                \n",
    "        #Creating a new data frame for Validation Dataset with two features\n",
    "        #It's predicted Random Forest and XGB values\n",
    "        X1 = pd.DataFrame({'forest_value': forest_val_value, 'xgb_value': xgb_val_value})\n",
    "        \n",
    "        #Training a XGB Model based on this new validation data frame\n",
    "        #Parameter values used are after tuning them accordingly\n",
    "        #obj = ParameterTuning(X1, Y1)\n",
    "        \n",
    "        model2 = XGBClassifier(\n",
    "         learning_rate =0.1,\n",
    "         n_estimators=1000,\n",
    "         max_depth=2,\n",
    "         min_child_weight=6,\n",
    "         gamma=0,\n",
    "         subsample=0.8,\n",
    "         colsample_bytree=0.8,\n",
    "         objective= 'binary:logistic',\n",
    "         nthread=4,\n",
    "         scale_pos_weight=1,\n",
    "         seed=27)\n",
    "\n",
    "        self.stackedModel = model2.fit(X1, Y2, verbose=True)\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class StartingClass:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Read the Featured Engineering Files\n",
    "        data = pd.read_csv('../FeatureEngineeringFiles/featured_train3.csv', engine='python')\n",
    "        test_X = pd.read_csv('../FeatureEngineeringFiles/featured_test3.csv', engine='python')\n",
    "        \n",
    "        #Cleaning the dataset of null values\n",
    "        cleaning = DataCleaning()\n",
    "        cleaning.clean_data(data)\n",
    "        cleaning.clean_data(test_X)\n",
    "        \n",
    "        #Dividing data set in training and validation\n",
    "        #Tried combinations of splits, 0.4 worked the best\n",
    "        train, val = train_test_split(data, train_size=0.4)\n",
    "        train = pd.DataFrame(train)\n",
    "        val = pd.DataFrame(val)\n",
    "        \n",
    "        #X contains training without is_duplicate\n",
    "        #Y contains the target column is_duplicate\n",
    "        X = train.iloc[0:,0:43]\n",
    "        Y = train.iloc[0:,43:44]\n",
    "        \n",
    "        #Model Training\n",
    "        model = ModelTraining()\n",
    "        model.layer_one_model_training(X, Y, val, test_X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pickle.dump(model.xgbModel, open(\"../PickleFiles/xgbModel.pkl\", 'wb'))\n",
    "        pickle.dump(model.randomModel, open(\"../PickleFiles/randomModel.pkl\", 'wb'))\n",
    "        pickle.dump(model.stackedModel, open(\"../PickleFiles/stackModel.pkl\", 'wb'))\n",
    "        \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#Starting Point\n",
    "if __name__ == '__main__':\n",
    "    #Read csv files\n",
    "    obj = StartingClass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from fuzzywuzzy import fuzz\n",
    "import pandas_ml as pdml\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Visualizations:\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Hello')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Various Function Definitions for Feature Engineering\n",
    "class FeatureEngineeringFunctions:\n",
    "    \n",
    "    #Given question2 column is not entirely string\n",
    "    #Converting question2 to string and in the test set \n",
    "    #Even question1 column might have such problem hence converting that too    \n",
    "    def convert_to_string(self, train):\n",
    "        train['question1'] = train['question1'].astype(str)\n",
    "        train['question2'] = train['question2'].astype(str)\n",
    "        train['lem_question1'] = train['lem_question1'].astype(str)\n",
    "        train['lem_question2'] = train['lem_question2'].astype(str)\n",
    "\n",
    "\n",
    "    #Porter Stemming\n",
    "    def portertok(self, text):\n",
    "        import nltk\n",
    "        from nltk.stem.porter import PorterStemmer\n",
    "        porter = PorterStemmer()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return \" \".join(porter.stem(word) for word in tokens) \n",
    "\n",
    "\n",
    "    #Counts of common words\n",
    "    #First convert to string then lower case then split on the basis of space then create a set\n",
    "    #then check the common elements of both set and take count of it\n",
    "    def count_common(self, question):\n",
    "        q1, q2 = question\n",
    "        return len((set(str(q1).lower().split(' ')) & set(str(q2).lower().split(' '))))/len((set(str(q1).lower().split(' ')).union(set(str(q2).lower().split(' ')))))\n",
    "\n",
    "\n",
    "    #Comparing each question pair\n",
    "    def check_each_pair(self, question1, question2):\n",
    "        print(question1, question2)\n",
    "\n",
    "    #Total unique words in both question pair\n",
    "    def total_unique_words(self, question):\n",
    "        q1, q2 = question\n",
    "        return len(set(str(q1)).union(set(str(q2))))\n",
    "\n",
    "\n",
    "    #Counts the total number of words present\n",
    "    def words_count(self, question):\n",
    "        return len(str(question).split(' '))\n",
    "\n",
    "\n",
    "    #Gives the length of the string\n",
    "    def length(self, question):\n",
    "        return len(str(question))\n",
    "\n",
    "    #Word count ratio\n",
    "    def word_count_ratio(self, question):\n",
    "        q1, q2 = question\n",
    "        l1 = len(set(str(q1)))*1.0 \n",
    "        l2 = len(set(str(q2)))\n",
    "        if l2 == 0:\n",
    "            return np.nan\n",
    "        if l1 / l2:\n",
    "            return l2 / l1\n",
    "        else:\n",
    "            return l1 / l2\n",
    "\n",
    "    #Remove Punctuations\n",
    "    def remove_punc(self, question):\n",
    "        return question.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Same First Word\n",
    "    def same_first_word(self, question):\n",
    "        q1, q2 = question\n",
    "        return floatset(set(str(q1).lower().split(' '))[0] == set(str(q2).lower().split(' '))[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    \n",
    "    #Print log_loss\n",
    "    def print_log_loss(target, predicted):\n",
    "        from sklearn.metrics import log_loss    \n",
    "        print('Predicted log loss score:', log_loss(target, predicted))\n",
    "\n",
    "\n",
    "    #Print Accuracy\n",
    "    def print_accuracy(target, predicted):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy = accuracy_score(target, predicted)\n",
    "        print(accuracy * 100.0)\n",
    "\n",
    "    #Print F_Score\n",
    "    def print_f_score(target, predicted):\n",
    "        from sklearn.metrics import f1_score\n",
    "        f1 = f1_score(test_Y, predicted)\n",
    "        print(\"F Score: %.2f%%\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "#Applying Feature Engineering\n",
    "class FeatureEngineering:\n",
    "    \n",
    "    def get_feature_engineered_data(self, flag):\n",
    "\n",
    "        #Creating an object of Data Exploration \n",
    "        obj = FeatureEngineeringFunctions()\n",
    "        \n",
    "        if flag==1:\n",
    "            train = pd.read_csv('../LematizedFiles/trainlem.csv', engine='python')\n",
    "        else:\n",
    "            train = pd.read_csv('../LematizedFiles/testlem.csv',engine='python')\n",
    "\n",
    "        obj.convert_to_string(train)\n",
    "\n",
    "        #Data Parameter used for some feature engineering attributes\n",
    "        EPSILON = 0.0000001\n",
    "\n",
    "        #Map works element wise on a series\n",
    "        #Apply works row/column wise on a dataframe\n",
    "        train['q1_word_num'] = train['question1'].map(obj.words_count)\n",
    "        train['q2_word_num'] = train['question2'].map(obj.words_count)\n",
    "\n",
    "        train['q1_length'] = train['question1'].map(obj.length)\n",
    "        train['q2_length'] = train['question2'].map(obj.length)\n",
    "\n",
    "\n",
    "        train['word_num_difference'] = abs(train.q1_word_num - train.q2_word_num)\n",
    "        train['length_difference'] = abs(train.q1_length - train.q2_length)\n",
    "\n",
    "        train['q1_has_fullstop'] = train.question1.apply(lambda x: int('.' in x))\n",
    "        train['q2_has_fullstop'] = train.question2.apply(lambda x: int('.' in x))\n",
    "\n",
    "        train['q1_digit_count'] = train.question1.apply(lambda question: sum([word.isdigit() for word in question]))\n",
    "        train['q2_digit_count'] = train.question2.apply(lambda question: sum([word.isdigit() for word in question]))\n",
    "        train['digit_count_difference'] = abs(train.q1_digit_count - train.q2_digit_count)\n",
    "\n",
    "        train['q1_capital_char_count'] = train.question1.apply(lambda question: sum([word.isupper() for word in question]))\n",
    "        train['q2_capital_char_count'] = train.question2.apply(lambda question: sum([word.isupper() for word in question]))\n",
    "        train['capital_char_count_difference'] = abs(train.q1_capital_char_count - train.q2_capital_char_count)\n",
    "\n",
    "        train['q1_has_math_expression'] = train.question1.apply(lambda x: int('[math]' in x))\n",
    "        train['q2_has_math_expression'] = train.question2.apply(lambda x: int('[math]' in x))      \n",
    "\n",
    "        train['common_words'] = train[['question1', 'question2']].apply(obj.count_common, axis=1)\n",
    "        train['lem_common_words'] = train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1)\n",
    "\n",
    "        train['log_word_share'] = np.log(train[['question1', 'question2']].apply(obj.count_common, axis=1) + EPSILON)\n",
    "        train['lem_log_word_share'] = np.log(train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1) + EPSILON)\n",
    "\n",
    "        train['word_share_squared'] = (train[['question1', 'question2']].apply(obj.count_common, axis=1) ** 2)\n",
    "        train['lem_word_share_squared'] = (train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1) ** 2)\n",
    "\n",
    "\n",
    "        train['word_share_sqrt'] = np.sqrt(train[['question1', 'question2']].apply(obj.count_common, axis=1))\n",
    "        train['lem_word_share_sqrt'] = np.sqrt(train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1))\n",
    "\n",
    "        train['log_length_difference'] = np.log(train.length_difference + EPSILON)\n",
    "        train['length_difference_squared'] = train.length_difference ** 2\n",
    "        train['length_difference_sqrt'] = np.sqrt(train.length_difference)\n",
    "\n",
    "        train['log_lem_tfidf'] = np.log(train.lem_tfidf_word_match + EPSILON)\n",
    "        train['lem_tfidf_squared'] = train.lem_tfidf_word_match ** 2\n",
    "        train['lem_tfidf_sqrt'] = np.sqrt(train.lem_tfidf_word_match)\n",
    "\n",
    "        train['total_unique_words'] = train[['question1', 'question2']].apply(obj.total_unique_words, axis=1)\n",
    "        train['word_count_ratio'] = train[['question1', 'question2']].apply(obj.word_count_ratio, axis=1)\n",
    "\n",
    "       \n",
    "        train['fuzz_qratio'] = train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_WRatio'] = train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_ratio'] = train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_token_set_ratio'] = train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_token_sort_ratio'] = train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_token_set_ratio'] = train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_token_sort_ratio'] = train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        features = ['q1_word_num', 'q2_word_num', 'word_num_difference', 'q1_length', 'q2_length',\n",
    "                'length_difference', 'q1_has_fullstop', 'q2_has_fullstop', 'q1_digit_count', 'q2_digit_count',\n",
    "                'digit_count_difference', 'q1_capital_char_count', 'q2_capital_char_count',\n",
    "                'capital_char_count_difference', 'q1_has_math_expression', 'q2_has_math_expression', \n",
    "                'log_length_difference', 'log_word_share', 'word_share_squared', 'word_share_sqrt', 'length_difference_squared', 'length_difference_sqrt', 'common_words',\n",
    "                'lem_common_words','lem_log_word_share','lem_word_share_squared','lem_word_share_sqrt','tfidf_word_match',\n",
    "                'lem_tfidf_word_match', 'intersection_count', 'log_lem_tfidf','lem_tfidf_squared','lem_tfidf_sqrt',\n",
    "                   'total_unique_words', 'word_count_ratio','fuzz_qratio','fuzz_WRatio','fuzz_partial_ratio','fuzz_partial_token_set_ratio',\n",
    "                   'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio','fuzz_token_sort_ratio']\n",
    "\n",
    "\n",
    "        if flag==1:\n",
    "            target = 'is_duplicate'\n",
    "            X = train[features]\n",
    "            Y = train[target]\n",
    "            return X,Y\n",
    "        else:\n",
    "            X = train[features]\n",
    "            return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cleaning null values of TF-IDF and lemmatizated features\n",
    "class DataCleaning:\n",
    "    \n",
    "    #Cleaning the first half of training dataset\n",
    "    def clean_data(self, X):\n",
    "        X['tfidf_word_match'] = X['tfidf_word_match'].fillna(0)\n",
    "        X['lem_tfidf_word_match'] = X['lem_tfidf_word_match'].fillna(0)\n",
    "        X['log_lem_tfidf'] = X['log_lem_tfidf'].fillna(0)\n",
    "        X['lem_tfidf_squared'] = X['lem_tfidf_squared'].fillna(0)\n",
    "        X['lem_tfidf_sqrt'] = X['lem_tfidf_sqrt'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameter Tuning for XGBoost based on the dataset\n",
    "class ParameterTuning:\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        self.get_best_learning_rate(X, Y)\n",
    "        self.get_best_depth_weight(X, Y)\n",
    "        \n",
    "        \n",
    "    #Best learning rate 0.3 for the first XGBoost    \n",
    "    def get_best_learning_rate(self, X, Y):\n",
    "    \n",
    "        n = Y.shape\n",
    "        Y=np.array(Y).reshape((n[0],))\n",
    "\n",
    "        model = XGBClassifier()\n",
    "        learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "        param_grid = dict(learning_rate=learning_rate)\n",
    "        kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "        grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "        grid_result = grid_search.fit(X, Y)\n",
    "        # summarize results\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        \n",
    "        #Best max depth 5 and min_child_weight 3 for the first XGBoost\n",
    "        def get_best_depth_weight(self, X, Y):\n",
    "\n",
    "            n = Y.shape\n",
    "            Y=np.array(Y).reshape((n[0],))\n",
    "\n",
    "            param_test1 = {\n",
    "             'max_depth':range(3,10,1),\n",
    "             'min_child_weight':range(1,6,1)\n",
    "            }\n",
    "\n",
    "            gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.3, n_estimators=140, max_depth=5,\n",
    "             min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "             objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "             param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "            gsearch1.fit(X,Y)\n",
    "            gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multi Layer Model Training using Stacking\n",
    "#Layer 1 - Random Forest + XGBoost\n",
    "#Layer2 - XGBoost\n",
    "class ModelTraining:\n",
    "        \n",
    "    def layer_one_model_training(self, X, Y, val, test_X):\n",
    "        \n",
    "        #Training a Random Forest Model based on training data set\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            n_jobs=8,\n",
    "            random_state=28,\n",
    "            max_features=0.78\n",
    "            )\n",
    "        model.fit(X, Y)\n",
    "        \n",
    "\n",
    "        #Training a XGB Model based on training data set\n",
    "        #Parameters used are the ones we got from parameter tuning\n",
    "        #obj = ParameterTuning(X, Y)\n",
    "        model1 = XGBClassifier(\n",
    "         learning_rate=0.3,\n",
    "         n_estimators=140,\n",
    "         max_depth=5,\n",
    "         min_child_weight=3,\n",
    "         gamma=0,\n",
    "         subsample=0.8,\n",
    "         colsample_bytree=0.8,\n",
    "         objective= 'binary:logistic',\n",
    "         nthread=4,\n",
    "         scale_pos_weight=1,\n",
    "         seed=27)\n",
    "        model1.fit(X, Y, verbose=True)\n",
    "\n",
    "\n",
    "        #Forest Values for validation and test data set\n",
    "        X2 = val.iloc[0:,0:43]\n",
    "        Y2 = val.iloc[0:,43:44]\n",
    "        \n",
    "        forest_val_value = model.predict_proba(X2)[:, 1]\n",
    "        forest_test_value = model.predict_proba(test_X)[:, 1]\n",
    "        \n",
    "        #XGB Values for validation and test data set\n",
    "        xgb_val_value = model1.predict_proba(X2)[:, 1]\n",
    "        xgb_test_value = model1.predict_proba(test_X)[:, 1]\n",
    "        self.layer_two_model_training(forest_val_value, xgb_val_value, forest_test_value, xgb_test_value, Y2)\n",
    "        \n",
    "        \n",
    "    def layer_two_model_training(self, forest_val_value, xgb_val_value, forest_test_value, xgb_test_value, Y2):\n",
    "                \n",
    "        #Creating a new data frame for Validation Dataset with two features\n",
    "        #It's predicted Random Forest and XGB values\n",
    "        X1 = pd.DataFrame({'forest_value': forest_val_value, 'xgb_value': xgb_val_value})\n",
    "        \n",
    "        #Training a XGB Model based on this new validation data frame\n",
    "        #Parameter values used are after tuning them accordingly\n",
    "        #obj = ParameterTuning(X1, Y1)\n",
    "        \n",
    "        model2 = XGBClassifier(\n",
    "         learning_rate =0.1,\n",
    "         n_estimators=1000,\n",
    "         max_depth=2,\n",
    "         min_child_weight=6,\n",
    "         gamma=0,\n",
    "         subsample=0.8,\n",
    "         colsample_bytree=0.8,\n",
    "         objective= 'binary:logistic',\n",
    "         nthread=4,\n",
    "         scale_pos_weight=1,\n",
    "         seed=27)\n",
    "\n",
    "        model2.fit(X1, Y2, verbose=True)\n",
    "        \n",
    "        \n",
    "        #Creating a data set for test data set\n",
    "        #Predicting values for that\n",
    "        X2 = pd.DataFrame({'forest_value': forest_test_value, 'xgb_value': xgb_test_value})\n",
    "        p = model2.predict_proba(X2)[:, 1]\n",
    "        \n",
    "        sub = pd.DataFrame({'is_duplicate': p})\n",
    "        sub.to_csv('../OutputFiles/output4.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting the results to a csv file\n",
    "#Writing the results to a csv file        \n",
    "class ConvertToCSV:\n",
    "    \n",
    "    def __init__(self, p):\n",
    "        print('Hello')\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class StartingClass:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Data Exploration\n",
    "        visual_obj = Visualizations()\n",
    "        \n",
    "        \"\"\"\n",
    "        #Feature Engineering\n",
    "        obj = FeatureEngineering()\n",
    "        X_total,Y_total = obj.get_feature_engineered_data(1)\n",
    "        test_X = obj.get_feature_engineered_data(0)\n",
    "        \n",
    "        \"\"\"\n",
    "        #Read the Featured Engineering Files\n",
    "        data = pd.read_csv('../FeatureEngineeringFiles/featured_train3.csv', engine='python')\n",
    "        test_X = pd.read_csv('../FeatureEngineeringFiles/featured_test3.csv', engine='python')\n",
    "        X_total = data.iloc[0:,0:43]\n",
    "        Y_total = data.iloc[0:,43:44]\n",
    "        \n",
    "        #Cleaning the dataset of null values\n",
    "        cleaning = DataCleaning()\n",
    "        cleaning.clean_data(X_total)\n",
    "        cleaning.clean_data(test_X)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        #Over sampling\n",
    "        data = pdml.ModelFrame(X, target=Y)\n",
    "        sampler = data.imbalance.over_sampling.SMOTE()\n",
    "        sampled_data = data.fit_sample(sampler)\n",
    "        #sampled_data = sampled_data[sampled_data.is_duplicate != 0]\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #Used in case we don't read data from the featured engineered files\n",
    "        \"\"\"\n",
    "        #data = X_total\n",
    "        #data['is_duplicate'] = Y_total\n",
    "           \n",
    "        \"\"\"\n",
    "        \n",
    "        #Dividing data set in training and validation\n",
    "        #Tried combinations of splits, 0.4 worked the best\n",
    "        train, val = train_test_split(data, train_size=0.4, random_state=1023)\n",
    "        train = pd.DataFrame(train)\n",
    "        val = pd.DataFrame(val)\n",
    "        \n",
    "        #X contains training without is_duplicate\n",
    "        #Y contains the target column is_duplicate\n",
    "        X = train.iloc[0:,0:43]\n",
    "        Y = train.iloc[0:,43:44]\n",
    "        \n",
    "        #Model Training\n",
    "        model = ModelTraining()\n",
    "        model.layer_one_model_training(X, Y, val, test_X)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-a22689291540>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#Read csv files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStartingClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-126-f4df8c96e27f>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampled_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#Dividing data set in training and validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m#Tried combinations of splits, 0.4 worked the best\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2968\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2970\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2972\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ModelFrame' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "#Starting Point\n",
    "if __name__ == '__main__':\n",
    "    #Read csv files\n",
    "    obj = StartingClass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

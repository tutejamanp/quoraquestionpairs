{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the join csv file\n",
    "if __name__ == '__main__':\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import string \n",
    "    import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('check6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apt_duplicate(data):\n",
    "    d1, d2=data\n",
    "    if d1<0.50:\n",
    "        if d1<d2:\n",
    "            return d1\n",
    "        else:\n",
    "            return d2\n",
    "    else:\n",
    "        if d1>d2:\n",
    "            return d1\n",
    "        else:\n",
    "            return d2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def round_off(data):\n",
    "    \n",
    "    if data>0.97:\n",
    "        return 0.999999\n",
    "    elif data < 0.0005:\n",
    "        return 0.000001\n",
    "    else:\n",
    "        return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding the appropriate value from two duplicate columns\n",
    "data['logistic_xgb_no_math_fuzzy'] = data[['logistic_xgb', 'removed_math_fuzzy']].apply(apt_duplicate, axis=1)\n",
    "#print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('check6.csv')\n",
    "data['tushar'] = data['removed_math_fuzzy'].apply(round_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Writing the result to a csv file\n",
    "#sub = pd.DataFrame({'id': data['id'], 'min_max_split40': data['min_max_split40']})\n",
    "data.to_csv('check6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Print log_loss\n",
    "def print_log_loss(target, predicted):\n",
    "    from sklearn.metrics import log_loss   \n",
    "    print('Predicted log loss score:', log_loss(target, predicted))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted log loss score: 0.238671991731\n",
      "Predicted log loss score: 0.245060153151\n",
      "Predicted log loss score: 0.244309995119\n",
      "Predicted log loss score: 0.23993722459\n",
      "Predicted log loss score: 0.240196374312\n",
      "Predicted log loss score: 0.234626013713\n",
      "Predicted log loss score: 0.271278288283\n",
      "Predicted log loss score: 0.248154312014\n",
      "Predicted log loss score: 0.238641605684\n",
      "Predicted log loss score: 0.205961810466\n"
     ]
    }
   ],
   "source": [
    "#Checking Log Loss value for first 100 entries\n",
    "data1 = pd.read_csv('../InputFiles/check6.csv')\n",
    "data1 = data1[0:1001]\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['split50'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['split35'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['split40'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['min_max'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['min_max_split40'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['split_50_40'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['latest'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['added_features'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['added_features_split40'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['logistic_blending'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['blending_xgb_tuning'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['question_count'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['total_unique_words'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['added_total_unique_words'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['word_count_ratio'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['minmax_word_count_ratio'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['fuzzy'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['removed_math_expression'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['removed_math_fuzzy'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['learning_rate_3'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['learning_rate_01'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['tushar'])\n",
    "\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['only_xgb'])\n",
    "\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['only_logistic'])\n",
    "\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['only_random_forest'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['logistic_xgb'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['logistic_xgb_no_math_fuzzy'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['tuned_forest_xgb'])\n",
    "\n",
    "#print_log_loss(data1['is_duplicate'], data1['tuned_forest_xgb_2_6'])\n",
    "#print_log_loss(data1['is_duplicate'], data1['double_tuned_forest_xgb'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['removed_8_features'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['removed_4_features'])\n",
    "print_log_loss(data1['is_duplicate'], data1['no_features_removed'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['sampling_xgb'])\n",
    "print_log_loss(data1['is_duplicate'], data1['sampling_features'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['sampling_random'])\n",
    "print_log_loss(data1['is_duplicate'], data1['oversampled_stack'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['sampled_tuned'])\n",
    "\n",
    "print_log_loss(data1['is_duplicate'], data1['thera_vadha'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def apt_duplicate_value(data):\n",
    "    d1, d2, d3=data\n",
    "    if d1<0.69:\n",
    "        if d1<d2 and d1<d3:\n",
    "            return d1\n",
    "        elif d2<d1 and d2<d3:\n",
    "            return d2\n",
    "        else:\n",
    "            return d3\n",
    "    else:\n",
    "        if d1>d2 and d1>d3:\n",
    "            return d1\n",
    "        elif d2>d1 and d2>d3:\n",
    "            return d2\n",
    "        else:\n",
    "            return d3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../InputFiles/check6.csv')\n",
    "data['ulu1'] = data[['no_features_removed', 'oversampled_stack']].apply(apt_duplicate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('../InputFiles/check6.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted log loss score: 0.219091646298\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('check7.csv')\n",
    "data = data[501:1001]\n",
    "print_log_loss(data['is_duplicate'], data['yo7'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from numpy import loadtxt\n",
    "from sklearn import preprocessing \n",
    "from fuzzywuzzy import fuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Various Function Definitions for Feature Engineering\n",
    "class FeatureEngineeringFunctions:\n",
    "    \n",
    "    #Given question2 column is not entirely string\n",
    "    #Converting question2 to string and in the test set \n",
    "    #Even question1 column might have such problem hence converting that too    \n",
    "    def convert_to_string(self, train):\n",
    "        train['question1'] = train['question1'].astype(str)\n",
    "        train['question2'] = train['question2'].astype(str)\n",
    "        train['lem_question1'] = train['lem_question1'].astype(str)\n",
    "        train['lem_question2'] = train['lem_question2'].astype(str)\n",
    "\n",
    "\n",
    "    #Porter Stemming\n",
    "    def portertok(self, text):\n",
    "        import nltk\n",
    "        from nltk.stem.porter import PorterStemmer\n",
    "        porter = PorterStemmer()\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return \" \".join(porter.stem(word) for word in tokens) \n",
    "\n",
    "\n",
    "    #Counts of common words\n",
    "    #First convert to string then lower case then split on the basis of space then create a set\n",
    "    #then check the common elements of both set and take count of it\n",
    "    def count_common(self, question):\n",
    "        q1, q2 = question\n",
    "        return len((set(str(q1).lower().split(' ')) & set(str(q2).lower().split(' '))))/len((set(str(q1).lower().split(' ')).union(set(str(q2).lower().split(' ')))))\n",
    "\n",
    "\n",
    "    #Comparing each question pair\n",
    "    def check_each_pair(self, question1, question2):\n",
    "        print(question1, question2)\n",
    "\n",
    "    #Total unique words in both question pair\n",
    "    def total_unique_words(self, question):\n",
    "        q1, q2 = question\n",
    "        return len(set(str(q1)).union(set(str(q2))))\n",
    "\n",
    "\n",
    "    #Counts the total number of words present\n",
    "    def words_count(self, question):\n",
    "        return len(str(question).split(' '))\n",
    "\n",
    "\n",
    "    #Gives the length of the string\n",
    "    def length(self, question):\n",
    "        return len(str(question))\n",
    "\n",
    "    #Word count ratio\n",
    "    def word_count_ratio(self, question):\n",
    "        q1, q2 = question\n",
    "        l1 = len(set(str(q1)))*1.0 \n",
    "        l2 = len(set(str(q2)))\n",
    "        if l2 == 0:\n",
    "            return np.nan\n",
    "        if l1 / l2:\n",
    "            return l2 / l1\n",
    "        else:\n",
    "            return l1 / l2\n",
    "\n",
    "    #Remove Punctuations\n",
    "    def remove_punc(self, question):\n",
    "        return question.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    #Same First Word\n",
    "    def same_first_word(self, question):\n",
    "        q1, q2 = question\n",
    "        return floatset(set(str(q1).lower().split(' '))[0] == set(str(q2).lower().split(' '))[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the csv file\n",
    "#Applying Feature Engineering\n",
    "class FeatureEngineering:\n",
    "    \n",
    "    def get_feature_engineered_data(self, flag):\n",
    "\n",
    "        #Creating an object of Data Exploration \n",
    "        obj = FeatureEngineeringFunctions()\n",
    "        \n",
    "        if flag==1:\n",
    "            train = pd.read_csv('../LematizedFiles/trainlem.csv', engine='python')\n",
    "        else:\n",
    "            train = pd.read_csv('../LematizedFiles/testlem.csv',engine='python')\n",
    "\n",
    "        obj.convert_to_string(train)\n",
    "\n",
    "        #Data Parameter used for some feature engineering attributes\n",
    "        EPSILON = 0.0000001\n",
    "\n",
    "        #Map works element wise on a series\n",
    "        #Apply works row/column wise on a dataframe\n",
    "        train['q1_word_num'] = train['question1'].map(obj.words_count)\n",
    "        train['q2_word_num'] = train['question2'].map(obj.words_count)\n",
    "\n",
    "        train['q1_length'] = train['question1'].map(obj.length)\n",
    "        train['q2_length'] = train['question2'].map(obj.length)\n",
    "\n",
    "\n",
    "        train['word_num_difference'] = abs(train.q1_word_num - train.q2_word_num)\n",
    "        train['length_difference'] = abs(train.q1_length - train.q2_length)\n",
    "\n",
    "        train['q1_has_fullstop'] = train.question1.apply(lambda x: int('.' in x))\n",
    "        train['q2_has_fullstop'] = train.question2.apply(lambda x: int('.' in x))\n",
    "\n",
    "        train['q1_digit_count'] = train.question1.apply(lambda question: sum([word.isdigit() for word in question]))\n",
    "        train['q2_digit_count'] = train.question2.apply(lambda question: sum([word.isdigit() for word in question]))\n",
    "        train['digit_count_difference'] = abs(train.q1_digit_count - train.q2_digit_count)\n",
    "\n",
    "        train['q1_capital_char_count'] = train.question1.apply(lambda question: sum([word.isupper() for word in question]))\n",
    "        train['q2_capital_char_count'] = train.question2.apply(lambda question: sum([word.isupper() for word in question]))\n",
    "        train['capital_char_count_difference'] = abs(train.q1_capital_char_count - train.q2_capital_char_count)\n",
    "\n",
    "        train['q1_has_math_expression'] = train.question1.apply(lambda x: int('[math]' in x))\n",
    "        train['q2_has_math_expression'] = train.question2.apply(lambda x: int('[math]' in x))      \n",
    "\n",
    "        train['common_words'] = train[['question1', 'question2']].apply(obj.count_common, axis=1)\n",
    "        train['lem_common_words'] = train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1)\n",
    "\n",
    "        train['log_word_share'] = np.log(train[['question1', 'question2']].apply(obj.count_common, axis=1) + EPSILON)\n",
    "        train['lem_log_word_share'] = np.log(train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1) + EPSILON)\n",
    "\n",
    "        train['word_share_squared'] = (train[['question1', 'question2']].apply(obj.count_common, axis=1) ** 2)\n",
    "        train['lem_word_share_squared'] = (train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1) ** 2)\n",
    "\n",
    "\n",
    "        train['word_share_sqrt'] = np.sqrt(train[['question1', 'question2']].apply(obj.count_common, axis=1))\n",
    "        train['lem_word_share_sqrt'] = np.sqrt(train[['lem_question1', 'lem_question2']].apply(obj.count_common, axis=1))\n",
    "\n",
    "        train['log_length_difference'] = np.log(train.length_difference + EPSILON)\n",
    "        train['length_difference_squared'] = train.length_difference ** 2\n",
    "        train['length_difference_sqrt'] = np.sqrt(train.length_difference)\n",
    "\n",
    "        train['log_lem_tfidf'] = np.log(train.lem_tfidf_word_match + EPSILON)\n",
    "        train['lem_tfidf_squared'] = train.lem_tfidf_word_match ** 2\n",
    "        train['lem_tfidf_sqrt'] = np.sqrt(train.lem_tfidf_word_match)\n",
    "\n",
    "        train['total_unique_words'] = train[['question1', 'question2']].apply(obj.total_unique_words, axis=1)\n",
    "        train['word_count_ratio'] = train[['question1', 'question2']].apply(obj.word_count_ratio, axis=1)\n",
    "\n",
    "       \n",
    "        train['fuzz_qratio'] = train.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_WRatio'] = train.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_ratio'] = train.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_token_set_ratio'] = train.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_partial_token_sort_ratio'] = train.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_token_set_ratio'] = train.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "        train['fuzz_token_sort_ratio'] = train.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        features = ['q1_word_num', 'q2_word_num', 'word_num_difference', 'q1_length', 'q2_length',\n",
    "                'length_difference', 'q1_has_fullstop', 'q2_has_fullstop', 'q1_digit_count', 'q2_digit_count',\n",
    "                'digit_count_difference', 'q1_capital_char_count', 'q2_capital_char_count',\n",
    "                'capital_char_count_difference', 'q1_has_math_expression', 'q2_has_math_expression', \n",
    "                'log_length_difference', 'log_word_share', 'word_share_squared', 'word_share_sqrt', 'length_difference_squared', 'length_difference_sqrt', 'common_words',\n",
    "                'lem_common_words','lem_log_word_share','lem_word_share_squared','lem_word_share_sqrt','tfidf_word_match',\n",
    "                'lem_tfidf_word_match', 'intersection_count', 'log_lem_tfidf','lem_tfidf_squared','lem_tfidf_sqrt',\n",
    "                   'total_unique_words', 'word_count_ratio','fuzz_qratio','fuzz_WRatio','fuzz_partial_ratio','fuzz_partial_token_set_ratio',\n",
    "                   'fuzz_partial_token_sort_ratio','fuzz_token_set_ratio','fuzz_token_sort_ratio']\n",
    "\n",
    "\n",
    "        if flag==1:\n",
    "            target = 'is_duplicate'\n",
    "            X = train[features]\n",
    "            Y = train[target]\n",
    "            return X,Y\n",
    "        else:\n",
    "            X = train[features]\n",
    "            return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class StartingClass:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        #Feature Engineering\n",
    "        obj = FeatureEngineering()\n",
    "        X_total,Y_total = obj.get_feature_engineered_data(1)\n",
    "        test_X = obj.get_feature_engineered_data(0)\n",
    "        \n",
    "        X_total['is_duplicate'] = Y_total\n",
    "        \n",
    "        X_total.to_csv('../FeatureEngineeringFiles/featured_train5.csv', index=False)\n",
    "        test_X.to_csv('../FeatureEngineeringFiles/featured_test5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../LematizedFiles/trainlem.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a22689291540>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#Read csv files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStartingClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-4039f951e559>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#Feature Engineering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureEngineering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mX_total\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_engineered_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mtest_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_engineered_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-09fe704bdf26>\u001b[0m in \u001b[0;36mget_feature_engineered_data\u001b[1;34m(self, flag)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../LematizedFiles/trainlem.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../LematizedFiles/testlem.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 655\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    656\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    970\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python-fwf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m                 \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFixedWidthFieldParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 972\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    973\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, **kwds)\u001b[0m\n\u001b[0;32m   1953\u001b[0m         f, handles = _get_handle(f, 'r', encoding=self.encoding,\n\u001b[0;32m   1954\u001b[0m                                  \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1955\u001b[1;33m                                  memory_map=self.memory_map)\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Gaurav Joshi\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# Python 3 and binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../LematizedFiles/trainlem.csv'"
     ]
    }
   ],
   "source": [
    "#Starting Point\n",
    "if __name__ == '__main__':\n",
    "    #Read csv files\n",
    "    obj = StartingClass()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
